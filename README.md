# spamnet

## Introduction

In our modern society, text intelligence has become a crucial tool used in a broad number of applications. Even if our language is a rather complex system of rules and facts, current algorithms are able to extract an excellent amount of useful information out of such kind of data. The key concept, allowing us to use those techniques in a wide range of tasks like security analysis, information extraction or the construction of complex classification system is commonly summarized under the term "big data": Not only our calculation power gives us the capabilities to find, process and interpret the gigantic amount of data. For the first time in the history of the humanity, we have these facts actually available in fractions of seconds;  our knowledge is not longer stored in libraries used by few privileged, but completely available.

But without a good algorithm, even this available data are just wasted bytes. In order to gain real results, the algorithms we may use are crucial. With the boom of Neural Networks in the last decades, new architectures were developed especially optimized for Text intelligence. One of those are Recurrent Neural Networks, which have the unique advantage of using not only the plain features as the foundation of there prediction but even their sequence. The aim of this paper is the critical evaluation of such models in comparison to other Python-based state-of-the-art technology. The benchmark of choice is the commonly used task of classifying comments according to their nature as real expressions of opinions or mostly unwanted messages like advertisements, often referred to as "Spam". Besides a focus on an optimal structure, this analysis will focus on the performance of the network in relation to the required preprocessing to conclude at the end, if the performance of recurrent neural networks is significantly better than comparable algorithms.

## Method

### Datasets

Every data mining algorithm is only that good as the data, which is feed into it. Therefore, in order to train that actual model for the prediction of Spam, a collection of X comment is used. These samples were extracted and hand-annotated by X using five highly popular videos on "YouTube" and contain beside the author, the date of writing and an Id the actual comment and label. Other than comparable dataset like X, all the X spam comments and Y non-spam comments are written in English. While this fact implies already a loss of potential useable information it simplifies the actual processing of the data dramatically: Beside some emoticons, the texts do not utilize advanced and therefore hardly handleable Unicode features. Some comments contain additional makeup data for formatting purposes in form of HTML tags. The data is divided into five comma-separated CSV files, where quotes are used to escape comma in the actual text.
Due to its nature, trained data mining models should have the ability to correctly classify data which vary from those they were trained on. A common problem is the so-called "overfitting": In this case, the neural networks learn not context behind the actual features but the features itself. A separation of training and testing data is therefore essential; in this paper, a ratio of 70% training and 30% testing data was used. Moreover, most machine learning algorithm contains a partly random element and may differ highly between different testing sets. To find a reliable outcome, all the following experiments were applied at least three times; the variance of the actual results is stated beside the used evaluation metric. Wherever applicable, a 3-fold cross validation where used, guaranteeing that the algorithm trains all the time on different data. If such a cross-validation was not usable due to given constraints, the data was at least shuffled before each trial. At the end, the following datasets were used:
- ...

This separation provides not only information of the actual generalizability of a model but also about the differences in performance with a different amount of samples.

### Preprocessing

Other than many other data mining tasks, Text intelligence is applied on unstructured data rather than structured. It is essential to understand the difference between both approaches and the steps needed to generated knowledge out of them: Like in the processing of images, the actual data is not simply represented by an ordered list of datasets containing a specific information in a precisely defined datatype which is required by the algorithm. Instead, it is a bunch of different words which are most likely highly different from the deterministic and unique nature numbers are. Even worse, the type of text itself is more comparable with Twitter tweets rather than reliable sources like or medical reports, which are often used as the foundation of natural language processing tasks. Extensive preprocessing is, therefore, necessary to standardize the data in a format a computer may understand it and gain good results in terms of precision and recall of the model. Given these constraints, before the actual evaluation of the algorithms, a data pipeline was developed utilizing the well known "sklearn" and "nltk" packages.

#### Step 1: Loading and Tokenization

In order to allow a flexible approach in the choice of the data sources, the above-mentioned datasets were constructed as instances of an abstract class "DataSource", encapsulating common logic in the actual parsing of the file and providing an convenient way to access training and test datasets represented as tuples of the actual comment and a boolean describing its type. The inclusion of other provided metadata like the name of the author or time of the comment shows no significant improvements in the classification in smaller experiments was therefore excluded.
After the actual process of loading the CSV files, the Unicode characters of no further interest and the HTML tags were removed in order to prepare the data for the following steps. While the actual tokenization, the conversion from a single string into a list of words, requires in the common case just a splitting at whitespace, the noisy nature of the comments requires a more elaborate strategy in the generation of further usable entities. In order to be able to process typical internet slang like URLs, emoticons and simple ASCII art like arrows, the TwitterTokenizer of the NLTK package was used. The resulting list of entities was afterward used as the starting point for further processing.

#### Step 2: Preprocessing

For a convenient integration into the Scikit-learn workflow, the different preprocessors were developed as subclasses of an overall "Preprocessor" subclass. Afterwards, the different algorithms were integrated into a single Scikit-learn "Transformer", where they were individually activatable using boolean switches. Using this architecture, the different preprocessors were accessible as hyperparameters for the optimization using a grid-search in the following steps in order to find their optimal combination. In the following, the complete preprocessing pipeline of the data is described.

Preprocessing 1: Standardization
In the first preprocessing step, some kind of standardization was performed to reduce the amount of lexically different but semantical equal data significantly. This process is important for two reasons: On the one hand, it extends the raw input with semantic information usable for the classification algorithm. As an example, many spam comments contain URL or YouTube channels being advertised. While humans have the ability to easily identify these entities through the presence of the characteristic parts "http" and (often) ".com", for an algorithm every of these parts looks completely different. On the other hand, further preprocessing algorithms may be irritated by a number of special chars used in the "noisy" way we humans use social media. Therefore, in this step, utilizing different regular expressions, URLs, numbers, and emoticons were replaced by a fixed constant. Afterwards, any other special character beside the normal punctuation got removed.

Preprocessing 2: Slang removal
In this step, the wide amount of slang words and spelling mistakes got reduced. For doing that, first multiple times repeated character like in "loooooove" got removed. Due to the fact that almost any English term contains maximal two times the same letters behind each other, the overall amount of proper English words were untouched by replacing only three or more occurrences. In a next step, common slang words and spelling mistakes gets removed. The basis of this was the collection generated by X (...). Even if this list was generated automatically and is not fully trustworthy; this step reduced again the amount of variance in the text.

Preprocessing 3: POS tagging and lemmatization
In the next step, all the words in the now rather cleaned sentences got reduced to their common form. The algorithm of choice was, in this case, the Stanford Lemmatizer. Other than the rather dumb stemmer, the algorithm utilized additional knowledge about the actual word it is applied to improve the actual result. Therefore, the sentences where POS-tagged according to their structure utilizing the Penrose corpus as their source of data. After the classification by the Perceptron Tagger provided by the NLTK package, this knowledge was used to lemmatize the comment word by word.

Preprocessing 4: Stop word removal
In this step, every word known for its minimal information value got removed after their successful consideration for the actual sentence structure in the POS-tagging step before. While words like "to" may be important and rather common in our natural way of communication, they do not carry any significant data for the actual classification of the elements. The deletion is, therefore, an easy and efficient way to reduce again the number of words the algorithms have to handle. 

Preprocessing 4: Stemming
Often, stemming is used as a less complicated alternative to a lemmatization: Instead of reducing a word to a valid English term for a phrase, its reduction to a word stem is normally easier to implement. Nevertheless, the usage besides the lemmatization has a reason: The combination of both approaches is used for an optimal reduction of features being misspelled or otherwise wrongly classified. In the end of this step, the comments consisted out of a list of not longer syntactic correct words.

Preprocessing 5: Lowercase
In the next step, the capitalization of all words was standardized to a complete lowercase variant. After the tagging and stemming, the type of the first character did not carry any interesting meaning. By lowercasing all the characters, equal words perceived by a human were also equal in the byte representation, the text algorithm are applied on.

Preprocessing 6: Finalization
In the final step, the last remaining punctation necessary in the former steps like dots, commas, and hyphens got removed. Only the actual lowercase characters remained and represented the source of features used in the vectorization and classification of the following algorithms.

#### Step 3: Vectorization

Neither Recurrent Neural Networks, not other data mining algorithm works on real words. While these are convenient to use for humans and allow us to communicate in a simple manner, the different encodings and size would result in a rather inefficient way of handling it. Even for us humans, it seems understandable, that a representation where each distinct word matches a different concept is an optimized solution. Therefore, every machine learning algorithm used in text processing needs a function which is able to transform the words in a unique manner into a binary representation. Classical way of handling this is a classical, hash function: \forall W:V(W) \rightarrow N \land \forall W2 \ne W: V(W2) \ne V(W).

The actual implementation of the function may differ and is an implementation detail: Beside a classical "Bag of Word"-approach, where each word is simply represented by a unique number getting incremented through its calculation, fast hash algorithms with low potential of hash collision may be used for the same task, eliminating the necessity of storing an actual list of already founded words. 

In the following experiment, after the vectorization of the preprocessed words, two different formats were used to store in that way generated data. Reason for this decision was the fact that RNNs requires a different input than other algorithms: While those just take a vector of values with a fixed length matching the number of words to be considered sequential, other machine learning algorithms get a (dense) vector with the length of all words as input, where an element unequal zero marks this word as "used". The  Python library scikit-learn provides an already developed class for the latter task, for the RNN, a custom Bag of word matching the above-stated requirements was developed.

### Algorithms

Recurrent neural networks are, like the name already included, a specialized family of artificial neural networks. With the increasing popularity of feed-forward neural networks in the last decades due to their often proven excellent performance in a wide range of problems, RNNs were conducted as a more powerful tool especially for tasks involving the use of sequences of data rather than only data itself. To understand their specialty is is rather important to understand the difference between them and classical feed-forward neuronal networks. While former generates a fixed output vector out of a fixed input vector and a fixed number of computational steps, RNNs work on sequences of these vectors. Foundation of this is their ability to have an interior state which gets adapted between different samples and allows therefore further consideration of spatial frequency and common pattern. This ability was successfully used in speech recognition, machine translation and even the generation of text. For a rough evaluation of the performance of a "typical" RNN in the classification of Spam, a rather simple architecture with only a single hidden layer was used. In order to reduce the source of possible mistakes and improve the performance of the network, the python toolkit Keras was utilized; using the highly optimized Tensorflow framework as its backend. 

Unlike the other data mining tools, the input was not handled as a dense vector with a length of the complete number of words, but as a 16 number long sequence of the actual word indices. Longer sentences were truncated, shorter comments padded with zeros. In the first Embedding layer, each integer of this input vector was itself turned into a dense vector. After this transformation, an optimal Dropout layer was added before the actual artificial neurons: RNNs tend to easily overfit the data, by adding a certain amount of random noise it is possible to reduce this danger and improve the prediction results; the actual amount given in percentage was designed to be one of the hyperparameters. 
The following layer with actual RNN neurons afterward and their numbers were designed in a flexible manner, too: Besides the classical, simple RNN neurons described above, LSTM and GRU neurons were evaluated in this experiment. Both were designed to face the "long-term dependency problem": While RNNs are able to easily find dependencies between near elements, further context commonly needed in language processing tasks are not possible. By adding additional complexity to the actual neurons, Hochreiter \& Schmidhuber (1997) created LSTMs which may deal with that kind of "long-time memory". GRUs, published recently by Y, target the same problem but try to handle it in a less complex way. Even if studies show both types of advanced cells perform rather similar, it seems worth to explore if the GRUs may be better in dealing with the rather small amount of samples provided in the experiment.
After the following Dropout layer, the results are directly mapped to a single neuron of a Dense layer used as output. Due to the nature of the binary classification, the sigmoid function is used as the activation function. For the Backpropagation as part of the training process and under respect of the binary output of the dense layer, Keras' "binary_crossentropy" and beside this the "Adam" optimizer with the default parameter described in its paper is used.

Beside the neuronal network architecture, two other families of classifier were used to generate a valid ground truth for the evaluation. 
Random forests are generally one of the best performing algorithms in the family of decision trees. Inspired by the human reasoning process, those trees classify upon a sequential ordered number of (often binary) decisions. While these approach is rather powerfull, it has in general a problem with overfitting. Random forests try to minimize the problem by evaluating not a single, but a specific amount of individual trees and generate its prediction according to the predictions generated by the different trees.
Naive Bayes is the the third and last utilized algorithm. Even if it assumes conditional independence between the features, these family of algorithm performs surprisingly well on the classification tasks. Different members like Multinomial Naive Bayes, binarized Multinomial Naive Bayes and Bernoulli Naive Bayes focus on different types of features; in the following, Multinomial Naive Bayes is used according to its focus on multiple occurrences of a word for a classification.

## Results

### Part 1: Evaluation on unprocessed data

As a preparation for the evaluation of the different classifier regarding their performance, a run without any preprocessing was performed. The gained results were not only be used to generate a measurement for the dependency of the algorithms in terms of preprocessing, but also to gain first insights into the valid range of successful parameters. Most of the algorithms are highly dependent on their hyperparameters; the optimal set depends itself on the actual data there are applied on. A commonly used try-and-error method to find such parameters without having knowledge over suitable heuristics is the Grid search: By a systematic generation and evaluation of all possible combination of parameters, this method is slow but guarantees to find a global optimum. In order to increase the reproducibility of the results, this evaluation is performed multiple times; the results of the evaluation are afterward averaged. In the following analysis, the F-Measure is used to describe this ability of the model to classify former unseen data. Unlike the commonly used accuracy considering only the rate of detected true-positives, this measurement combines precision and recall and is therefore far better suitable for contexts, where the ratio between the classes is not exactly equal. While the term "recall" describes the capability of an algorithm to correctly classify all its targets in a set,  "precision" states how correct the algorithm was when it classified a sample as belonging to its target. 
In the following table, the averaged F-scores, precisions and recalls of the best performing model on the different datasets are summarized. Due to the fact that all scores where averaged independently from each other, the precision and recall may not result in the presented F-Score value but may be used to evaluate the performance in more details. The variance describes the diversity of the F-Score between the three trials the analysis was performed and states, how the generated model depends on the distribution of the samples in training set.

| Type  | Dataset       | F-Score | Variance | Precision | Recall |
| ----- | ------------- | ------- | -------- | --------- | ------ |
| Bayes | Single        | 81.4%   | +/- 5.8% | 83.6%     | 82.9%  |
|       | Splitted      | 76.4%   | -        | 71.7%     | 73.7%  |
|       | Multi         | 89.8%   | +/- 2.1% | 89.5%     | 89.4%  |
|       | MultiSplitted | 84.5%   | -        | 84.7%     | 83.4%  |
| RF    | Single        | 81.6%   | +/- 1.9% | 86.1%     | 82.9%  |
|       | Splitted      | 68%     | +/- 0.5% | 64.3%     | 65.7%  |
|       | Multi         | 89%     | +/- 1.8% | 90.3%     | 89.7%  |
|       | MultiSplitted | 87.1%   | +/- 0.2% | 86.8%     | 86.8%  |
| RNN   | Single        | 77.7%   | +/- 8.7% | 77.1%     | 76.9%  |
|       | Splitted      | 67.9%   | +/- 5%   | 67.8%     | 68%    |
|       | Multi         | 88.9%   | +/- 5.6% | 88.6%     | 88.4%  |
|       | Multisplitted | 81.1%   | +/- 6.3% | 80.5%     | 78.9%  |

With an average F-score from 84.35% considering all datasets, the Random Forest algorithm performed in this part of the experiment slightly better than the Naive Bayes approach with its 83.025%; the RNN followed with 78.27% performance. Due to two reasons, it is rather hard to find a clear winner after that round: On the one hand, the measured difference in performance at least between Random Forest and Naive Bayes is statistically insignificant. On the other hand, the variance up to 9% in the calculation leads to a rather high risk of getting results which are not reproducible. Normally, we may use two different methods utilizing the "law of the large numbers" to reduce the risk: One may increase the amount of data to reduce the influence of a single sample towards the prediction or repeat the experiments multiple time to get a classical Gaussian distribution of performance. Unfortunately, in this case, none of both methods is applicable due to the constraints regarding the datasets and the performance of available computers. It is, therefore, meaningfull to have a look at the performance of the four datasets in order to find tendencies allowing further research.

When being applied to a small, single dataset, the Random Forest algorithm performed best. Foundation of this claim is not the again statistical insignificant better performance of 81.6% in comparison to the 81.4% derived by Naive Bayes but the actual variance. Almost three times smaller than the variance of Naive Bayes, the value indicates the general stability over different training sets rather important in production environments. In general, it seems that the Random forest was better in classifying "real" spam (as shown by the high precision) rather than find them among all the other comments (measured by the recall). In absolute terms, the RNN performed worst at this dataset. Not only the rather small F-score shows the inappropriateness of the algorithm for the task, but the high variance between the different training sets. It seems it was in general not able to find the common pattern underlying the data but learned more the unstable noise of the training data.

The performance on the split dataset, where the model was trained on the comments below one video for classifying the spam under another one, differed highly. This types of tasks are interesting under the premise, that they actually measure the generalizability of a model even in terms of former unseen data from a different source; a situation, similar to those the algorithms have to face in real-world applications. Unlike the first dataset, the training data was only shuffled and not part of a cross-validation. This time, Naive Bayes performed constantly and clearly best. The determinism of its performance can be seen by the completely missing variance. Therefore, the actual interpretation of Multinomial Naive Bayes seems to be independent of the order of the samples. The Random Forest algorithm and the RNN performed with 68% rather similar. For interpreting the clearly visible gap in the performance of former, one may consider the common problem of tree-based algorithms with overfitting: It seems that the extracted decisions used for the classification seem to utilize more the noise in the training data than actually the model behind the concept "spam". The extreme variance in the performance of the RNN, based solely on the different order of the samples, shows the inability of this technique to extract useful patterns for classification on small datasets.

The 3-fold cross-validated dataset containing 70% of all available samples for training allowed the algorithms to develop their full strength. On the unprocessed data, all algorithms were capable of showing a good performance around 89%. Like it would be expected, the increased amount of available samples reduced the significance of unique samples and lead therefore to a lower variance on all models. Nevertheless, the actual benefit in comparison to the first dataset depended highly on the algorithm: While Naive Bayes profited highly and the RNN shows a significant drop, too, the results for the Random Forest algorithm were rather subtle.

Surprisingly, on the last dataset being an extension of the second one utilizing three datasets for training, Random Forest performed clearly best. After its bad performance on the second dataset due to its overfitting, the algorithm was capable of creating a far better generalizing model on the increased number of samples; the reached F-score of 87.1% was even comparable to the result on the third dataset. For Naive Bayes, the magnitude of the drop in terms of performance was comparable with the difference in it between the first pair of datasets. Even if the recurrent neural network performed not that successful in terms of the F-score, it seems to be capable of compensating the additional difficulty until a certain extent with the additional data in a way, that the actual difference was not that high like between the first two datasets.

### Part 2: Evaluation on preprocessed data

After the detailed evaluation of the results on the unprocessed data, in the next step, the different preprocessing algorithms were additionally considered into the analysis. Due to their nature as independent parts of a data processing pipeline, the different preprocessing steps were added as optimizable hyperparameter. Using this approach, the actual evaluation becomes again a grid search, this time over the 128 permutations of activated and deactivated steps. As a starting point for this analysis, the optimal set of parameters gained in the experiment before was used. While this dependency on former results is rather critical and a clear drawback due to the possibility of another optimum, the exponentially rising number of permutations in combination with the limited available computational power especially in the training of neural networks required such kind of simplification.

| Type  | Dataset       | F-Score | Variance | Precision | Recall |
| ----- | ------------- | ------- | -------- | --------- | ------ |
| Bayes | Single        | 97.2%   | +/- 4.3% | 97.2%     | 97.1%  |
|       | Splitted      | 93.6%   | -        | 92.1%     | 89.1%  |
|       | Multi         | 93.6%   | +/- 1.9% | 93.4%     | 93.3%  |
|       | MultiSplitted | 88%     | -        | 88.7%     | 86.6%  |
| RF    | Single        | 98.2%   | +/- 2.5% | 98.4%     | 98.3%  |
|       | Splitted      | 97%     | -        | 95.4%     | 96.4%  |
|       | Multi         | 96.4%   | +/- 1.7% | 96.4%     | 96.4%  |
|       | MultiSplitted | 94%     | +/- 0.9% | 94%       | 93.9%  |
| RNN   | Single        | 92.7%   | +/- 0.9% | 93.0%     | 92.9%  |
|       | Splitted      | 95.1%   | +/- 1.4% | 92.9%     | 93.2%  |
|       | Multi         | 95.5%   | +/- 0.6% | 95.5%     | 95.5%  |
|       | MultiSplitted | 94.1%   | +/- 1.4% | 94.5%     | 93.7%  |

In consideration of the performances in the first experiment, the actual importance of the preprocessing got clearly evident. For all classifier, the performance was at least improved by 10%, while the recurrent neural network benefited the most. With an average performance of 96.40%, the Random Forest algorithm performed again best, this time followed by the RNN with 94.35% performance. The Naive Bayes implementation was mostly unsuccessful in gaining performance and performed with 93.10%. Besides this increased ability of model generation, a factor worth mentioning is the influence of preprocessing on the variance of the algorithms. Again, the differences between the three different evaluations got reduced for all algorithms, and again mostly beneficial for the recurrent neural network. By this notable change, the algorithm evolves from the "problem child" of the experiment into one of the stablest algorithms, leaving even Naive Bayes behind it. This development is not only advantageous in terms of the suitability for a usage in productive contexts but offers also additional confidence in terms of a credible interpretation of the results.

Even if most of the notable correlations and conclusions between the different results remains the same in comparison with the first experiment, other results show clear differences. As an example, the two machine learning algorithms used as a baseline for the evaluation of the recurrent neural network performed on the preprocessed data best on the small dataset evaluated with a 3-fold cross-validation; the 98.20% performance reached by the Random Forest algorithm is even the best value gained in the complete experiment. It is not trivial to explain, why an increase of data did not lead to a better model like it would be assumed. One explanation may be, that the preprocessing of the data biased the process of vectorization towards inner features of a single dataset in some way. Unlike these two algorithms, the RNN behaved in completely different manner: While its performance on the single dataset was not comparable to the performance of the other algorithms, it was the only one which was able to improve its performance on the second, split dataset requiring a more generalized model. Again, an explanation of this effect is not trivial: It seems, that instead of overfitting the data, the RNN generated on the small number of preprocessed samples a too general model being able to show its strength only if it is applied to the completely unknown data. 
On the other two bigger datasets, such a correlation was no longer evident. Common for all algorithms was the drop of performance between the third and the fourth dataset. While these decreases were dramatically for Naive Bayes which falls from 93.60% to 88%, the subtle changes in the performance of the RNN made it to the slightly best performing algorithm on the last dataset.

| Type  | Dataset       | Standartization | Slang | Lemma | Stopword | Stemming | Lowercase |      |
| ----- | ------------- | :-------------: | :---: | :---: | :------: | :------: | :-------: | ---- |
| Bayes | Single        |        ✓        |       |       |          |    ✓     |           |      |
|       | Splitted      |        ✓        |       |       |    ✓     |          |           | ✓    |
|       | Multi         |        ✓        |   ✓   |   ✓   |          |    ✓     |     ✓     |      |
|       | MultiSplitted |        ✓        |   ✓   |       |          |          |           |      |
| RF    | Single        |        ✓        |   ✓   |       |          |    ✓     |           | ✓    |
|       | Splitted      |        ✓        |   ✓   |       |    ✓     |          |           | ✓    |
|       | Multi         |        ✓        |   ✓   |       |    ✓     |    ✓     |     ✓     |      |
|       | MultiSplitted |        ✓        |   ✓   |   ✓   |    ✓     |          |     ✓     |      |
| RNN   | Single        |        ✓        |   ✓   |   ✓   |    ✓     |          |     ✓     | ✓    |
|       | Splitted      |        ✓        |   ✓   |       |    ✓     |    ✓     |     ✓     | ✓    |
|       | Multi         |        ✓        |   ✓   |   ✓   |    ✓     |    ✓     |     ✓     | ✓    |
|       | MultiSplitted |        ✓        |   ✓   |   ✓   |    ✓     |    ✓     |     ✓     |      |

The actual evaluation of the optimal set of the parameters shows some commonalities and some differences: Clearly evident is the actual import role of the standardization of the input by a replacement of URLs, numbers, and emoticons. This crucial enrichment and normalization of words with semantic meaning is also performed by the removal of spelling mistakes and common slang, too. 
While the probabilistic model of Multinomial Naive Bayes seems to be able to handle stopwords itself, for both other algorithms the removal of that bloat seems to be important. A context insensitive capitalization of words seems not that important; only the neural network relies on that. A possible explanation might be the removal of slang performed before, which already reduced the number of words differing in their capitalization significantly. The combination of Lemmatization and Stemming seems to be meaningful only for complex datasets with the recurrent neural network; the other algorithms seems to require normally just one of them. Most of the time, even the computational far less expensive Stemming algorithm seems to be completely sufficient.
To summarize the results, one may claim that the recurrent neural network works clearly best when applied which as preprocessed and standardized data as possible, not only evident due to the increase of performance and the decrease of variance. For the other algorithms applies the well-known rule, that finding the actual optimal preprocessing pipeline is a matter of experimentation. 

## Evaluation and further research

Are recurrent neuronal networks capable of significantly enhancing the way we deal with Spam? According to the result mentioned above, it is trivial to claim the performance of the Recurrent Neural Networks in the classification of spam are not significantly better than other algorithms. In comparison with approaches only taking into the account the occurrences of words and not the semantic structure of sentences, the additional semantic features seem to have no evidential positive influence for the general performance. While this result may not be exactly the result one may wish, some of the results we might conclude from the experiment may have relevance for further research.

First of all, Recurrent Neural Networks seem to be highly depended on the number of available samples. In the experiments, an increase of the sample size resulted clearly in an increase of performance and reduction of an otherwise uncontrollable variance. In order to further evaluate the performance of recurrent neural networks, it would, therefore, be highly adequate to apply it on bigger datasets. Even if such resources exist, further preprocessing like a limitation towards the English language would be necessary. While also other algorithms like Random Forest and Naive Bayes profit from the additional data, they seem to be able to compensate a smaller number of training samples quite well. 

Secondly, Recurrent Neural Networks needs extensive preprocessing. Even if deep learning technology is known of being capable to deal even with contradicting data, the data itself needs to be in a fairly ordered state. At least with the amount of data used in the experiment, the baseline algorithms seem to be far more robust when working on unprocessed data.

Thirdly, the amount of time the complex neural networks needs to train are even in our times of exponential raising computer power not trivial. On the first glance, it seems acceptable to train a neuronal network twenty minutes on a relatively small dataset of just 2000 samples. But even a 3-fold validation gets in such situations extremely inefficient, further optimizations of hyperparameters become simply impossible. While additional research may apply the same code just to more powerful hardware to solve such a problem, it seems hard to believe that the scaling of computational power will scale with the computational complexity of deep learning techniques forever. If other algorithms are capable of generating similar results in a fraction of that time, we should simply for the sake of efficiency take care of the choice of our algorithms.

Finally, as an overall conclusion, the experiment showed again in a clear manner that there is no silver bullet for any task in machine learning or data mining. Even if we are nowadays able to archive result with our deep learning technology we were unable to imagine only a few years ago, we have to critically evaluate the performance and the efficiency of our tools over and over again. "For a man with a hammer, everything looks like a nail" - according to this slogan it remains necessary keeping our open-minded, unbiased view towards our available tools in order to reach our goals in an efficient and optimal way.