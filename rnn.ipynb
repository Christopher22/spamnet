{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "class Data(ABC):\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, split_important=True):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.split_important = split_important\n",
    "    \n",
    "    def training_size(self):\n",
    "        return len(self.x_train)\n",
    "    \n",
    "    def testing_size(self):\n",
    "        return len(self.x_test)\n",
    "    \n",
    "    def find_optimum(self, pipeline, parameters):\n",
    "        grid = GridSearchCV(\n",
    "            pipeline, \n",
    "            n_jobs = 1, \n",
    "            verbose = 1, \n",
    "            scoring = {\n",
    "                'f1': make_scorer(f1_score, pos_label=True),\n",
    "                'precision': 'precision_macro',\n",
    "                'recall': 'recall_macro'\n",
    "            }, \n",
    "            refit = 'f1', \n",
    "            cv=(VariationGenerator(self, 3) if self.split_important else None), \n",
    "            param_grid=parameters\n",
    "        )\n",
    "\n",
    "        print(\"Training: {} (Training size: {}, Testing size: {})\".format(\n",
    "            self.__class__.__name__, \n",
    "            self.training_size(), \n",
    "            self.testing_size()\n",
    "        ))\n",
    "              \n",
    "        grid.fit([x for x in self.x_train + self.x_test], [y for y in self.y_train + self.y_test])  \n",
    "        for mean, std, prec, recall, params in sorted(\n",
    "            zip(grid.cv_results_['mean_test_f1'], \n",
    "                grid.cv_results_['std_test_f1'],\n",
    "                grid.cv_results_['mean_test_precision'],\n",
    "                grid.cv_results_['mean_test_recall'],\n",
    "                grid.cv_results_['params'],\n",
    "            ), key = lambda x: x[0], reverse = True)[:3]:\n",
    "\n",
    "            print(\"{0:1.3f} (+/-{1:1.3f}; Precision: {2:1.3f}, Recall: {3:1.3f}) for {4}\".format(\n",
    "                mean, \n",
    "                std * 2, \n",
    "                prec, \n",
    "                recall, \n",
    "                params\n",
    "            ))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def _load_file(file):\n",
    "         with open(file, encoding='ascii', errors='ignore') as spam:\n",
    "            for row in csv.DictReader(spam, delimiter=',', quotechar='\"', skipinitialspace=True, strict=True):\n",
    "                yield [row['CONTENT'].strip(), row['CLASS'] == '1']\n",
    "                    \n",
    "class VariationGenerator:\n",
    "    def __init__(self, data, n):\n",
    "        self.n = n\n",
    "        self.training_size = data.training_size()\n",
    "        self.testing_size = data.testing_size()\n",
    "\n",
    "    def split(self, *_):\n",
    "        for _ in range(self.n):\n",
    "            yield (\n",
    "                shuffle(list(range(self.training_size))), \n",
    "                shuffle(list(range(self.training_size, self.training_size + self.testing_size)))\n",
    "            )\n",
    "            \n",
    "    def get_n_splits(self, *_):\n",
    "        return self.n\n",
    "        \n",
    "class SingleFile(Data):\n",
    "    def __init__(self, path, test_ratio=0.3):\n",
    "        data = [comment for comment in Data._load_file(path)]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            [comment[0] for comment in data], \n",
    "            [comment[1] for comment in data], \n",
    "            test_size=test_ratio\n",
    "        )\n",
    "        super().__init__(x_train, y_train, x_test, y_test, False)\n",
    "        \n",
    "    \n",
    "class SplittedFile(Data):\n",
    "    def __init__(self, training, testing, test_ratio=0.3):\n",
    "        training = [comment for comment in Data._load_file(training)]\n",
    "        testing = [comment for comment in Data._load_file(testing)]\n",
    "        super().__init__(\n",
    "            [comment[0] for comment in training],\n",
    "            [comment[1] for comment in training],\n",
    "            [comment[0] for comment in testing[:int(len(training) * test_ratio)]],\n",
    "            [comment[1] for comment in testing[:int(len(training) * test_ratio)]],\n",
    "            True\n",
    "        )\n",
    "\n",
    "class MixedFiles(Data):\n",
    "    def __init__(self, paths, test_ratio=0.3):\n",
    "        data = []\n",
    "        for file in glob.iglob(paths):\n",
    "            data.extend(comment for comment in Data._load_file(file))\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            [comment[0] for comment in data], \n",
    "            [comment[1] for comment in data], \n",
    "            test_size=test_ratio\n",
    "        )\n",
    "        super().__init__(x_train, y_train, x_test, y_test, False)\n",
    "        \n",
    "class SplittedMixedFiles(Data):\n",
    "    def __init__(self, paths, test_ratio=0.3):\n",
    "        files = glob.glob(paths)\n",
    "        shuffle(files)\n",
    "        \n",
    "        testing = [comment for comment in Data._load_file(files[0])]\n",
    "        training = []\n",
    "        for file in files[1:]:\n",
    "            training.extend(comment for comment in Data._load_file(file))\n",
    "            \n",
    "        super().__init__(\n",
    "            [comment[0] for comment in training],\n",
    "            [comment[1] for comment in training],\n",
    "            [comment[0] for comment in testing],\n",
    "            [comment[1] for comment in testing],\n",
    "            True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "class Preprocessor(ABC):\n",
    "    @abstractmethod\n",
    "    def optimize(self, tokenized_comment):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @staticmethod    \n",
    "    def preprocess(comments, preprocessors):\n",
    "        tokenizer = TweetTokenizer()\n",
    "        html_cleaner = re.compile('<.+?>')\n",
    "        for comment in comments:\n",
    "            comment = html_cleaner.sub('', comment)\n",
    "            tokenized_comment = tokenizer.tokenize(comment)\n",
    "            for preprocessor in preprocessors:\n",
    "                tokenized_comment = preprocessor.optimize(tokenized_comment)\n",
    "            yield tokenized_comment\n",
    "\n",
    "class StandardizePreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        # \\/?watch \\? v = \\S+\n",
    "        self.regex_url = re.compile(r'[^\\s|^\\.]+\\.[a-z]{2,3}[^\\s]*')\n",
    "        self.regex_number = re.compile(r'\\b[0-9]+\\b')\n",
    "        self.regex_emoji = re.compile(r'[\\S]{0,3}:[\\S]{1,3}')\n",
    "        self.regex_special = re.compile(r'&[a-z]+;')\n",
    "        \n",
    "    def optimize(self, tokenized_comment):   \n",
    "        return [self.regex_emoji.sub('EMOJII', \n",
    "                                     self.regex_number.sub('NUM', \n",
    "                                                           self.regex_url.sub('URL', self.regex_special.sub('', word))\n",
    "                                                          )\n",
    "                                    ) \n",
    "                for word in tokenized_comment]\n",
    "\n",
    "class SlangPreprocessor(Preprocessor):\n",
    "    def __init__(self, normalisation_dictionary):\n",
    "        self.double_character = re.compile(r'(.)\\1{2,}')\n",
    "        \n",
    "        self.dictionary = {}\n",
    "        with open(normalisation_dictionary, encoding='ascii', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                key, value = line.strip().split(\"\\t\")\n",
    "                self.dictionary[key] = value\n",
    "            \n",
    "    def optimize(self, tokenized_comment):\n",
    "        output = []\n",
    "        for word in tokenized_comment:\n",
    "            word = self.double_character.sub(r'\\1\\1', word)\n",
    "            if word.lower() in self.dictionary:\n",
    "                word = self.dictionary[word.lower()]\n",
    "            output.append(word)\n",
    "        output = list(OrderedDict.fromkeys(output))\n",
    "        return output\n",
    "    \n",
    "class PosLemmatizationPreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        self.regex_non_word = re.compile(r\"[^a-zA-Z\\.!?']\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _tag_to_wordnet(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def optimize(self, tokenized_comment):\n",
    "        output = []\n",
    "        for word in tokenized_comment:\n",
    "            word = self.regex_non_word.sub('', word).strip()\n",
    "            if len(word) > 0:\n",
    "                output.append(word)\n",
    "                \n",
    "        for i, (word, tag) in enumerate(pos_tag(output)):\n",
    "            pos_type = self._tag_to_wordnet(tag)\n",
    "            if pos_type is not None:\n",
    "                output[i] = self.lemmatizer.lemmatize(word, pos=pos_type)\n",
    "            else:\n",
    "                output[i] = word\n",
    "                \n",
    "        return output\n",
    "\n",
    "class StemmerPreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "        \n",
    "    def optimize(self, tokenized_comment):\n",
    "        return [self.porter.stem(word) for word in tokenized_comment]\n",
    "    \n",
    "class StopwordPreprocessor(Preprocessor):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def optimize(self, tokenized_comment):\n",
    "        return [word for word in tokenized_comment if word.lower() not in self.stop_words]\n",
    "    \n",
    "class LowercasePreprocessor(Preprocessor):       \n",
    "    def optimize(self, tokenized_comment):\n",
    "        return [word.lower() for word in tokenized_comment]\n",
    "\n",
    "class PunctationRemover(Preprocessor):\n",
    "    def __init__(self):\n",
    "        self.char_only = re.compile(r'[^a-zA-Z]')\n",
    "        \n",
    "    def optimize(self, tokenized_comment):\n",
    "        output = []\n",
    "        for word in tokenized_comment:\n",
    "            word = self.char_only.sub('', word)\n",
    "            if len(word) > 0:\n",
    "                output.append(word)\n",
    "        return output\n",
    "    \n",
    "class PreprocessorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                use_standartize=True,\n",
    "                use_slang=True,\n",
    "                use_stopword=True,\n",
    "                use_lemmatization=True,\n",
    "                use_stemmer=True,\n",
    "                use_lowercase=True,\n",
    "                use_punctation=True):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        preprocessors = []\n",
    "        if self.use_standartize:\n",
    "            preprocessors.append(StandardizePreprocessor())\n",
    "        if self.use_slang:\n",
    "            preprocessors.append(SlangPreprocessor('dictionaries/slang.txt'))\n",
    "        if self.use_stopword:\n",
    "            preprocessors.append(StopwordPreprocessor())\n",
    "        if self.use_lemmatization:\n",
    "            preprocessors.append(PosLemmatizationPreprocessor())\n",
    "        if self.use_stemmer:\n",
    "            preprocessors.append(StemmerPreprocessor())\n",
    "        if self.use_lowercase:\n",
    "            preprocessors.append(LowercasePreprocessor())\n",
    "        if self.use_punctation:\n",
    "            preprocessors.append(PunctationRemover())\n",
    "        return [tokenized for tokenized in Preprocessor.preprocess(X, preprocessors)]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class BagOfWords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_occurrences=2, max_features=None):\n",
    "        self.counter = Counter()\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.max_features = max_features\n",
    "        self.bow = None\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        for x in X:\n",
    "            self.counter.update(x)\n",
    "            \n",
    "        self.bow = {}\n",
    "        i = 2\n",
    "        for word, occurences in self.counter.most_common(self.max_features):\n",
    "            if occurences >= self.min_occurrences:\n",
    "                self.bow[word] = i\n",
    "                i += 1\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        if self.bow is None:\n",
    "            raise RuntimeError(\"Fitting required before transform!\")\n",
    "        \n",
    "        output = []\n",
    "        for x in X:\n",
    "            output.append([self.bow[word] if word in self.bow else 1 for word in x])\n",
    "        return output\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RnnClassifier(BaseEstimator, ClassifierMixin):  \n",
    "\n",
    "    def __init__(self, input_length=32, \n",
    "                 embedding_dimension=32, \n",
    "                 batch_size=32, \n",
    "                 epochs=3, \n",
    "                 num_hidden_neurons=100,\n",
    "                 dropout=0,\n",
    "                 rnn_type='gru',\n",
    "                 num_words=2000):\n",
    "        \n",
    "        self.input_length = input_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.dropout = dropout\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_words = num_words\n",
    "        self._rnn = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        assert (y is not None), \"Y is required\"\n",
    "        assert (self.rnn_type in ['gru', 'lstm', 'simple']), \"Invalid RNN type\"\n",
    "        \n",
    "        X = pad_sequences(X, self.input_length)\n",
    "        X = np.clip(X, 0, self.num_words - 1)\n",
    "        \n",
    "        self._rnn = Sequential()\n",
    "        self._rnn.add(Embedding(self.num_words, self.embedding_dimension, input_length=self.input_length))\n",
    "        if self.dropout > 0:\n",
    "            self._rnn.add(Dropout(self.dropout))\n",
    "        \n",
    "        if self.rnn_type is 'gru':\n",
    "            self._rnn.add(GRU(self.num_hidden_neurons))\n",
    "        elif self.rnn_type is 'lstm':\n",
    "            self._rnn.add(LSTM(self.num_hidden_neurons))\n",
    "        else:\n",
    "            self._rnn.add(SimpleRNN(self.num_hidden_neurons))\n",
    "            \n",
    "        if self.dropout > 0:\n",
    "            self._rnn.add(Dropout(self.dropout))\n",
    "        self._rnn.add(Dense(1))\n",
    "        self._rnn.add(Activation('sigmoid'))\n",
    "        self._rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self._rnn.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        if self._rnn is None:\n",
    "            raise RuntimeError(\"Fitting required before prediction!\")\n",
    "\n",
    "        X = pad_sequences(X, self.input_length)\n",
    "        return [prob[0] >= 0.5 for prob in self._rnn.predict(X, batch_size=self.batch_size)]\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        assert (y is not None), \"Y is required\"\n",
    "        \n",
    "        prediction = self.predict(X)\n",
    "        return f1_score(y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def dummy(x):\n",
    "    return x\n",
    "            \n",
    "preprocessor = PreprocessorTransformer(use_standartize=False, \n",
    "                                       use_slang=False, \n",
    "                                       use_stopword=False, \n",
    "                                       use_lemmatization=False,\n",
    "                                       use_stemmer=False,\n",
    "                                       use_lowercase=False,\n",
    "                                       use_punctation=False)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=dummy, preprocessor=dummy, max_features=2000)\n",
    "\n",
    "rnn_pipeline = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"bow\", BagOfWords(max_features=2000)),\n",
    "    (\"rnn\", RnnClassifier())\n",
    "], memory='cache')\n",
    "\n",
    "gaussian_pipeline = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"gaussian\", MultinomialNB())\n",
    "], memory='cache')\n",
    "\n",
    "forest_pipeline = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"forest\", RandomForestClassifier())\n",
    "], memory='cache')\n",
    " \n",
    "datasets = [\n",
    "    SingleFile('data/Youtube01-Psy.csv'),\n",
    "    SplittedFile('data/Youtube01-Psy.csv', 'data/Youtube02-KatyPerry.csv'),\n",
    "    MixedFiles('data/*.csv'),\n",
    "    SplittedMixedFiles('data/*.csv')\n",
    "]\n",
    "\n",
    "print(\"RNN:\")\n",
    "for data in datasets:\n",
    "    data.find_optimum(rnn_pipeline, {\n",
    "        \"rnn__epochs\": [3, 10, 20],\n",
    "        \"rnn__num_hidden_neurons\": [50, 100, 200],\n",
    "        \"rnn__dropout\": [0, 0.1, 0.2],\n",
    "        \"rnn__rnn_type\": ['gru', 'lstm', 'simple']\n",
    "    })\n",
    "\n",
    "print(\"\\nGaussian:\")\n",
    "for data in datasets:\n",
    "    data.find_optimum(gaussian_pipeline, {\n",
    "        \"gaussian__alpha\": [0.5, 0.75, 1.0, 1.25, 1.5],\n",
    "        \"gaussian__fit_prior\": [True, False],\n",
    "    })\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "for data in datasets:\n",
    "    data.find_optimum(forest_pipeline, {\n",
    "        \"forest__n_estimators\": [10, 100, 500, 800],\n",
    "        \"forest__max_features\": ['sqrt', 'log2', None],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: SingleFile (Training size: 245, Testing size: 105)\n",
      "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "forest_pipeline = Pipeline([\n",
    "    (\"pre\", PreprocessorTransformer()),\n",
    "    (\"vectorizer\", CountVectorizer(tokenizer=dummy, preprocessor=dummy)),\n",
    "    (\"forest\", RandomForestClassifier(n_estimators=100))\n",
    "], memory='cache')\n",
    " \n",
    "datasets = [\n",
    "    SingleFile('data/Youtube01-Psy.csv'),\n",
    "    SplittedFile('data/Youtube01-Psy.csv', 'data/Youtube02-KatyPerry.csv'),\n",
    "    MixedFiles('data/*.csv'),\n",
    "    SplittedMixedFiles('data/*.csv')\n",
    "]\n",
    "       \n",
    "for data in datasets:\n",
    "    data.find_optimum(forest_pipeline, {\n",
    "        \"pre__use_standartize\": [True, False],\n",
    "        \"pre__use_slang\": [True, False],\n",
    "        \"pre__use_stopword\": [True, False],\n",
    "        \"pre__use_lemmatization\": [True, False],\n",
    "        \"pre__use_stemmer\": [True, False],\n",
    "        \"pre__use_lowercase\": [True, False],\n",
    "        \"pre__use_punctation\": [True, False]\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
